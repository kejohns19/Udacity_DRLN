{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "#env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\")\n",
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from buffer import ReplayBuffer\n",
    "from common.Memory import ReplayMemory\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor\n",
    "from collections import deque\n",
    "\n",
    "# keep training awake\n",
    "#from workspace_utils import keep_awake\n",
    "\n",
    "# for saving gif\n",
    "#import imageio\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# number of parallel agents\n",
    "#parallel_envs = 4\n",
    "# number of training episodes.\n",
    "# change this to higher number to experiment. say 30000.\n",
    "number_of_episodes = 2000\n",
    "episode_length = 200\n",
    "batchsize = 128\n",
    "# how many episodes to save policy and gif\n",
    "save_interval = 1000\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "noise = 6\n",
    "noise_reduction = 0.9999\n",
    "\n",
    "# how many episodes before update\n",
    "episode_per_update = 10\n",
    "\n",
    "log_path = os.getcwd()+\"/log\"\n",
    "model_dir= os.getcwd()+\"/model_dir\"\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "#torch.set_num_threads(parallel_envs)\n",
    "#env = envs.make_parallel_env(parallel_envs)\n",
    "\n",
    "# keep 5000 episodes worth of replay\n",
    "buffer = ReplayMemory(int(1e5))\n",
    "\n",
    "# initialize policy and critic\n",
    "\n",
    "in_actor = state_size\n",
    "hidden_in_actor = 256\n",
    "hidden_out_actor = 256\n",
    "out_actor = 2\n",
    "# critic input = obs from both agents + actions from both agents\n",
    "in_critic = 2*state_size + 2*action_size\n",
    "hidden_in_critic = 512\n",
    "hidden_out_critic = 256\n",
    "\n",
    "maddpg = MADDPG(in_actor, hidden_in_actor, hidden_out_actor, \n",
    "                out_actor, in_critic, hidden_in_critic, hidden_out_critic)\n",
    "logger = SummaryWriter(log_dir=log_path)\n",
    "agent0_reward = []\n",
    "agent1_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 0/2000   0% ETA:  --:--:-- |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 0 is -9.900989877705526e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 20/2000   1% ETA:  0:09:06 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 25 is -0.0004761904064151976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 40/2000   2% ETA:  0:05:20 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 50 is 0.0005960266247687751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 79/2000   3% ETA:  0:03:26 |/                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 75 is 0.00022727289152416316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 95/2000   4% ETA:  0:03:12 |-                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 100 is -0.0005472635025557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 112/2000   5% ETA:  0:02:58 |\\\\                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 125 is -0.0002654865219266014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 149/2000   7% ETA:  0:02:37 |//                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 150 is -0.00043824679437149094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 169/2000   8% ETA:  0:02:27 |---                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 175 is -0.0013043476079685101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 204/2000  10% ETA:  0:02:17 ||||                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 200 is -0.0016943519359510207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 219/2000  10% ETA:  0:02:15 |////                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 225 is -0.0011042942367265561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 254/2000  12% ETA:  0:02:08 |\\\\\\\\                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 250 is -0.0014529912082473096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 272/2000  13% ETA:  0:02:05 ||||||                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 275 is -0.002021276352411889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 291/2000  14% ETA:  0:02:02 |/////                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 300 is -0.0020199498751588595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 322/2000  16% ETA:  0:01:59 |\\\\\\\\\\\\                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 325 is -0.00201877908772426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 339/2000  16% ETA:  0:01:57 |||||||                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 350 is -0.001352549619361461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 376/2000  18% ETA:  0:01:52 |-------                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 375 is -0.0015966383849873261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 391/2000  19% ETA:  0:01:51 |\\\\\\\\\\\\\\                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 400 is -0.0014171653916081506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 427/2000  21% ETA:  0:01:46 |////////                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 425 is -0.0014448666399762657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 447/2000  22% ETA:  0:01:44 |--------                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 450 is -0.0016515423694182653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 465/2000  23% ETA:  0:01:42 |\\\\\\\\\\\\\\\\\\                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 475 is -0.0016666663836480842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 502/2000  25% ETA:  0:01:39 |/////////                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 500 is -0.0016805321604211794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 517/2000  25% ETA:  0:01:38 |----------                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 525 is -0.001533546035710615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 536/2000  26% ETA:  0:01:36 |\\\\\\\\\\\\\\\\\\\\                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 550 is -0.0017050688345265645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 575/2000  28% ETA:  0:01:32 |///////////                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 575 is -0.0018639050357969556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 594/2000  29% ETA:  0:01:30 |-----------                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 600 is -0.001868758624294685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 613/2000  30% ETA:  0:01:28 |\\\\\\\\\\\\\\\\\\\\\\                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 625 is -0.0018732779436121303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 653/2000  32% ETA:  0:01:25 |////////////                           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 650 is -0.001877496376157442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 673/2000  33% ETA:  0:01:23 |-------------                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 675 is -0.0018814430024820504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 692/2000  34% ETA:  0:01:22 |\\\\\\\\\\\\\\\\\\\\\\\\\\                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 700 is -0.001885143272606621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 712/2000  35% ETA:  0:01:20 ||||||||||||||                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 725 is -0.0020096849325421935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 752/2000  37% ETA:  0:01:17 |--------------                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 750 is -0.0022444180360657066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 772/2000  38% ETA:  0:01:15 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 775 is -0.0023515978785546405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 792/2000  39% ETA:  0:01:13 ||||||||||||||||                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 800 is -0.0023418421010496086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 811/2000  40% ETA:  0:01:12 |///////////////                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 825 is -0.002548595817988944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 851/2000  42% ETA:  0:01:09 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 850 is -0.0027444792028073883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 869/2000  43% ETA:  0:01:08 |||||||||||||||||                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 875 is -0.0025204915064768714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 885/2000  44% ETA:  0:01:07 |/////////////////                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 900 is -0.002507492209543715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 924/2000  46% ETA:  0:01:04 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 925 is -0.0025925922950050754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 944/2000  47% ETA:  0:01:03 |||||||||||||||||||                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 950 is -0.0027687913311962623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 977/2000  48% ETA:  0:01:01 |-------------------                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 975 is -0.002843865875473253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 991/2000  49% ETA:  0:01:00 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1000 is -0.0030063575625988055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1020/2000  51% ETA:  0:00:59 |///////////////////                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1025 is -0.0030728238626494078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1048/2000  52% ETA:  0:00:58 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1050 is -0.003049521859926208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1064/2000  53% ETA:  0:00:57 |||||||||||||||||||||                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1075 is -0.0030272105886113075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1100/2000  55% ETA:  0:00:54 |--------------------                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1100 is -0.003089092125950209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1117/2000  55% ETA:  0:00:53 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1125 is -0.003148449947951182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1154/2000  57% ETA:  0:00:51 |/////////////////////                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1150 is -0.003205435355004075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1174/2000  58% ETA:  0:00:50 |----------------------                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1175 is -0.00333855769868508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1203/2000  60% ETA:  0:00:48 |||||||||||||||||||||||                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1200 is -0.003466563887728076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1219/2000  60% ETA:  0:00:47 |///////////////////////               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1225 is -0.003514328514974491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1246/2000  62% ETA:  0:00:46 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1250 is -0.003634344637504249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1261/2000  63% ETA:  0:00:45 ||||||||||||||||||||||||               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1275 is -0.003749999709069989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1296/2000  64% ETA:  0:00:43 |------------------------              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1300 is -0.003647394425140629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1312/2000  65% ETA:  0:00:42 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1325 is -0.0037587654866999194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1348/2000  67% ETA:  0:00:39 |/////////////////////////             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1350 is -0.003866298813539731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1365/2000  68% ETA:  0:00:38 |-------------------------             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1375 is -0.0039024387339892264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1402/2000  70% ETA:  0:00:36 |||||||||||||||||||||||||||            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1400 is -0.004003997045822894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1421/2000  71% ETA:  0:00:35 |//////////////////////////            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1425 is -0.004102227758972626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1453/2000  72% ETA:  0:00:33 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1450 is -0.004197291782465694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1470/2000  73% ETA:  0:00:32 ||||||||||||||||||||||||||||           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1475 is -0.004289339815365709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1501/2000  75% ETA:  0:00:30 |----------------------------          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1500 is -0.0042535912182687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1518/2000  75% ETA:  0:00:29 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1525 is -0.004341943133370389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1548/2000  77% ETA:  0:00:27 |/////////////////////////////         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1550 is -0.004367049986542824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1579/2000  78% ETA:  0:00:25 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1575 is -0.00433174195657162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1596/2000  79% ETA:  0:00:24 |||||||||||||||||||||||||||||||        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1600 is -0.004415049684671428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1629/2000  81% ETA:  0:00:22 |------------------------------        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1625 is -0.004495944095039492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1645/2000  82% ETA:  0:00:21 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1650 is -0.004517418332929818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1678/2000  83% ETA:  0:00:19 |///////////////////////////////       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1675 is -0.004538288003311971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1692/2000  84% ETA:  0:00:18 |--------------------------------      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1700 is -0.004558578282511942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1725/2000  86% ETA:  0:00:16 |||||||||||||||||||||||||||||||||      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1725 is -0.004633077481498397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1741/2000  87% ETA:  0:00:15 |/////////////////////////////////     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1750 is -0.004705564276406341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1774/2000  88% ETA:  0:00:13 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1775 is -0.004776119120490513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1791/2000  89% ETA:  0:00:12 |||||||||||||||||||||||||||||||||||    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1800 is -0.004844818234851271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1824/2000  91% ETA:  0:00:10 |----------------------------------    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1825 is -0.00491173388310713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1841/2000  92% ETA:  0:00:09 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1850 is -0.00497693462494946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1870/2000  93% ETA:  0:00:08 |///////////////////////////////////   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1875 is -0.005040485550449626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1902/2000  95% ETA:  0:00:06 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1900 is -0.005102448496801862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1919/2000  95% ETA:  0:00:05 |||||||||||||||||||||||||||||||||||||  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1925 is -0.005162882249019047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1950/2000  97% ETA:  0:00:03 |------------------------------------- | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1950 is -0.005221842725950587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1966/2000  98% ETA:  0:00:02 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward 1975 is -0.005279383152859691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2000/2000 100% Time: 0:02:04 |||||||||||||||||||||||||||||||||||||||| \n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "# show progressbar\n",
    "import progressbar as pb\n",
    "widget = ['episode: ', pb.Counter(),'/',str(number_of_episodes),' ', \n",
    "          pb.Percentage(), ' ', pb.ETA(), ' ', pb.Bar(marker=pb.RotatingMarker()), ' ' ]\n",
    "\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=number_of_episodes).start()\n",
    "'''\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "actions = np.random.randn(num_agents, action_size)\n",
    "actions = np.clip(actions, -1, 1)\n",
    "env_info = env.step(actions)[brain_name] \n",
    "'''\n",
    "\n",
    "scores_deque = deque(np.zeros(100))\n",
    "\n",
    "# initialize buffer with random actions\n",
    "\n",
    "for _ in range(1500):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    obs = env_info.vector_observations\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    actions = actions.astype(np.float32)\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_obs = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    \n",
    "    # add data to buffer\n",
    "    transition = (obs, actions, rewards, next_obs, dones)\n",
    "    buffer.push(*transition)\n",
    "\n",
    "for episode in range(0, number_of_episodes):\n",
    "\n",
    "    timer.update(episode)\n",
    "\n",
    "    reward_this_episode = np.zeros(num_agents)\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    obs = env_info.vector_observations\n",
    "\n",
    "    #for calculating rewards for this particular episode - addition of all time steps\n",
    "\n",
    "    # save info or not\n",
    "    save_info = ((episode) % save_interval == 0 or episode==number_of_episodes-1)\n",
    "    #frames = []\n",
    "    tmax = 0\n",
    "    '''\n",
    "    if save_info:\n",
    "        frames.append(env.render('rgb_array'))\n",
    "    '''\n",
    "    #for episode_t in range(episode_length):\n",
    "    while True:\n",
    "        #t += 1 #parallel_envs\n",
    "\n",
    "        # explore = only explore for a certain number of episodes\n",
    "        # action input needs to be transposed\n",
    "        actions = maddpg.act(torch.tensor(obs, dtype=torch.float), noise=noise)\n",
    "        noise *= noise_reduction\n",
    "\n",
    "        actions = torch.stack(actions).detach().numpy()\n",
    "\n",
    "        # step forward one frame\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_obs = env_info.vector_observations            # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    " \n",
    "        # add data to buffer\n",
    "        transition = (obs, actions, rewards, next_obs, dones)\n",
    "        buffer.push(*transition)\n",
    "\n",
    "        reward_this_episode += rewards\n",
    "\n",
    "        obs = next_obs\n",
    "        '''\n",
    "        # save gif frame\n",
    "        if save_info:\n",
    "            frames.append(env.render('rgb_array'))\n",
    "            tmax+=1\n",
    "        '''\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    \n",
    "    scores_deque.append(np.sum(reward_this_episode))\n",
    "    \n",
    "    # update once after every episode_per_update\n",
    "    if len(buffer) > batchsize and episode % episode_per_update == 0:\n",
    "        samples = buffer.sample(batchsize)\n",
    "        for a_i in range(num_agents):\n",
    "            #samples = buffer.sample(batchsize)\n",
    "            maddpg.update(samples, a_i, logger)\n",
    "        maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "\n",
    "    agent0_reward.append(reward_this_episode[0])\n",
    "    agent1_reward.append(reward_this_episode[1])\n",
    "\n",
    "    if episode % 100 == 0 or episode == number_of_episodes-1:\n",
    "        avg_rewards = [np.mean(agent0_reward), np.mean(agent1_reward)]\n",
    "        agent0_reward = []\n",
    "        agent1_reward = []\n",
    "        for a_i, avg_rew in enumerate(avg_rewards):\n",
    "            logger.add_scalar('agent%i/mean_episode_rewards' % a_i, avg_rew, episode)\n",
    "        print('last 100 avg reward {} is {}'.format(episode, np.mean(scores_deque)))\n",
    "\n",
    "    #saving model\n",
    "    save_dict_list =[]\n",
    "    if save_info:\n",
    "        for i in range(2):\n",
    "\n",
    "            save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                         'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                         'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                         'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "            save_dict_list.append(save_dict)\n",
    "\n",
    "            torch.save(save_dict_list, \n",
    "                       os.path.join(model_dir, 'episode-{}.pt'.format(episode)))\n",
    "        '''\n",
    "        # save gif files\n",
    "        imageio.mimsave(os.path.join(model_dir, 'episode-{}.gif'.format(episode)), \n",
    "                        frames, duration=.04)\n",
    "        '''\n",
    "        \n",
    "logger.close()\n",
    "timer.finish()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
