{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#notebook { padding-top:0px !important; } .container { width:100% !important; } .end_space { min-height:0px !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\n",
    "    '<style>'\n",
    "        '#notebook { padding-top:0px !important; } ' \n",
    "        '.container { width:100% !important; } '\n",
    "        '.end_space { min-height:0px !important; } '\n",
    "    '</style>'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "#env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\")\n",
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from buffer import ReplayBuffer\n",
    "from common.Memory import ReplayMemory\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor\n",
    "from collections import deque\n",
    "\n",
    "# keep training awake\n",
    "#from workspace_utils import keep_awake\n",
    "\n",
    "# for saving gif\n",
    "#import imageio\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# number of training episodes.\n",
    "# change this to higher number to experiment. say 30000.\n",
    "number_of_episodes = 60000\n",
    "episode_length = 200\n",
    "batchsize = 256\n",
    "# how many episodes to save policy and gif\n",
    "save_interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "noise = 2\n",
    "noise_reduction = 0.999\n",
    "\n",
    "# how many episodes before update\n",
    "episode_per_update = 1\n",
    "\n",
    "log_path = os.getcwd()+\"/log\"\n",
    "model_dir= os.getcwd()+\"/model_dir\"\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# keep 5000 episodes worth of replay\n",
    "buffer = ReplayMemory(int(1e5))\n",
    "\n",
    "# initialize policy and critic\n",
    "\n",
    "in_actor = state_size\n",
    "hidden_in_actor = 256\n",
    "hidden_out_actor = 256\n",
    "out_actor = 2\n",
    "# critic input = obs from both agents + actions from both agents\n",
    "in_critic = 2*state_size + 2*action_size\n",
    "hidden_in_critic = 256\n",
    "hidden_out_critic = 256\n",
    "\n",
    "maddpg = MADDPG(in_actor, hidden_in_actor, hidden_out_actor, \n",
    "                out_actor, in_critic, hidden_in_critic, hidden_out_critic,\n",
    "                lr_actor=1.0e-5, lr_critic=1.0e-5, discount_factor=0.995, tau=1.0e-3)\n",
    "\n",
    "logger = SummaryWriter(log_dir=log_path)\n",
    "agent0_reward = []\n",
    "agent1_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 0/60000   0% ETA:  --:--:-- |                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 0 is -4.950494938852763e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 101/60000   0% ETA:  6:55:45 |                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 100 is 0.0037064678105178162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 200/60000   0% ETA:  8:05:59 |                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 200 is 0.003305648017266264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 300/60000   0% ETA:  8:49:25 |                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 300 is 0.003715710921161936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 400/60000   0% ETA:  9:04:52 |                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 400 is 0.002774451290463616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 500/60000   0% ETA:  9:11:05 |                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 500 is 0.0018136441124953367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 600/60000   1% ETA:  9:16:48 |                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 600 is 0.0008416549540450331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 700/60000   1% ETA:  9:23:03 |                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 700 is 0.00011235971780454323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 800/60000   1% ETA:  9:26:24 |                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 800 is -0.0002885680939262107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 900/60000   1% ETA:  9:33:40 |                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 900 is 0.0004395606158139346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1001/60000   1% ETA:  9:47:19 |                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 1000 is 0.0006721164417080074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1101/60000   1% ETA:  9:53:31 |                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 1100 is 0.00024146562384799755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1200/60000   2% ETA:  9:55:21 |                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 1200 is -0.00016141412343019526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1300/60000   2% ETA:  9:55:48 |                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 1300 is -0.0005067807019320579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1401/60000   2% ETA:  9:58:00 |                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 1400 is 0.00012658245744882307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1501/60000   2% ETA:  9:59:04 |                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 1500 is 0.00018113696986086884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1601/60000   2% ETA:  10:02:13 |                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 1600 is 0.001228689202936974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1701/60000   2% ETA:  10:03:25 ||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 1700 is 0.0016601889873588502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1800/60000   3% ETA:  10:04:42 |-                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 1800 is 0.0019410838481275926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1900/60000   3% ETA:  10:07:13 |-                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 1900 is 0.002543728353212709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2000/60000   3% ETA:  10:07:21 ||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 2000 is 0.0031128036530279706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2100/60000   3% ETA:  10:07:35 |-                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 2100 is 0.003562017497959894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2201/60000   3% ETA:  10:09:25 |-                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 2200 is 0.004167753393182621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2301/60000   3% ETA:  10:07:43 ||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 2300 is 0.0038067474259200914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2401/60000   4% ETA:  10:06:13 |-                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 2400 is 0.0034746103884666407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2501/60000   4% ETA:  10:04:50 ||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 2500 is 0.0032449060347774818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2601/60000   4% ETA:  10:04:01 |-                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 2600 is 0.003143280494567841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2701/60000   4% ETA:  10:02:04 ||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 2700 is 0.0028704036514632875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2801/60000   4% ETA:  10:00:08 |-                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 2800 is 0.0026163394138812813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 2901/60000   4% ETA:  9:58:22 ||                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 2900 is 0.002362546034936844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3001/60000   5% ETA:  9:57:14 |-                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 3000 is 0.002125121142219071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3101/60000   5% ETA:  9:57:05 ||                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 3100 is 0.0019025306695398968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3201/60000   5% ETA:  9:55:43 |-                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 3200 is 0.001708573367197255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3301/60000   5% ETA:  9:54:22 |||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 3300 is 0.0015113204046733341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3401/60000   5% ETA:  9:52:39 |--                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 3400 is 0.00132533582047126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3501/60000   5% ETA:  9:51:06 |||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 3500 is 0.001149680843833866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3601/60000   6% ETA:  9:49:34 |--                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 3600 is 0.0009835181653125161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3701/60000   6% ETA:  9:48:19 |||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 3700 is 0.0008260985901072069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3801/60000   6% ETA:  9:48:43 |--                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 3800 is 0.0006767497442126029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3901/60000   6% ETA:  9:48:59 |||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 3900 is 0.000534866474218754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4001/60000   6% ETA:  9:49:13 |--                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 4000 is 0.0007290906595078453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4101/60000   6% ETA:  9:49:26 |||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 4100 is 0.0009616759887917982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4201/60000   7% ETA:  9:49:19 |--                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 4200 is 0.0011718207073801341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4301/60000   7% ETA:  9:48:50 |||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 4300 is 0.0011451944742259506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4401/60000   7% ETA:  9:48:16 |--                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 4400 is 0.001008664939401084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4501/60000   7% ETA:  9:47:49 |||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 4500 is 0.0009650078046904183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4601/60000   7% ETA:  9:47:47 |--                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 4600 is 0.0009232080252110083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4701/60000   7% ETA:  9:47:30 |||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 4700 is 0.000903978534710144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4801/60000   8% ETA:  9:47:24 |--                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 4800 is 0.0009253215602656283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 4901/60000   8% ETA:  9:47:28 ||||                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 4900 is 0.0009368128351639257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5001/60000   8% ETA:  9:47:46 |\\\\\\                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 5000 is 0.0009576555599280139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5101/60000   8% ETA:  9:47:33 |///                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 5100 is 0.0009488561875592445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5201/60000   8% ETA:  9:47:22 |\\\\\\                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 5200 is 0.0009592531696580375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5301/60000   8% ETA:  9:47:00 |///                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 5300 is 0.00095075005956837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5401/60000   9% ETA:  9:46:36 |\\\\\\                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 5400 is 0.0009243775840047554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5501/60000   9% ETA:  9:46:21 |---                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 5500 is 0.000916800769489911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5601/60000   9% ETA:  9:46:03 ||||                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 5600 is 0.0008831785874494522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5701/60000   9% ETA:  9:45:46 |---                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 5700 is 0.0008507155911369872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5800/60000   9% ETA:  9:45:27 ||||                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 5800 is 0.0008193528488937464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 5900/60000   9% ETA:  9:44:46 |---                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 5900 is 0.0007390436883832669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6000/60000  10% ETA:  9:44:34 ||||                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 6000 is 0.0007023440731649339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6100/60000  10% ETA:  9:43:57 |---                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 6100 is 0.0006265120469352119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6200/60000  10% ETA:  9:43:25 ||||                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 6200 is 0.0005530870045566158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6300/60000  10% ETA:  9:43:14 |---                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 6300 is 0.00046633357707969174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6400/60000  10% ETA:  9:43:23 ||||                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 6400 is 0.0004360869471278856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6500/60000  10% ETA:  9:43:19 |----                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 6500 is 0.0003688836927653948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6601/60000  11% ETA:  9:43:08 |////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 6600 is 0.00029622463331820624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6701/60000  11% ETA:  9:42:27 |\\\\\\\\                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 6700 is 0.0002477578711985595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6801/60000  11% ETA:  9:42:09 |////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 6800 is 0.00020069573919395832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 6901/60000  11% ETA:  9:41:36 |\\\\\\\\                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 6900 is 0.00014783621048258127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7001/60000  11% ETA:  9:40:36 |////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 7000 is 0.00010350673479012177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7101/60000  11% ETA:  9:39:52 |\\\\\\\\                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 7100 is 8.818238312470352e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7201/60000  12% ETA:  9:39:52 |\\\\\\\\                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 7200 is 5.958106513179144e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7301/60000  12% ETA:  9:39:35 |////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 7300 is 0.00010606693424794855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7401/60000  12% ETA:  9:39:14 |\\\\\\\\                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 7400 is 0.0001313160114619052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7501/60000  12% ETA:  9:38:34 |////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 7500 is 0.00025457195575445706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7601/60000  12% ETA:  9:37:26 |\\\\\\\\                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 7600 is 0.00031619289206163617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7701/60000  12% ETA:  9:36:28 |////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 7700 is 0.00029932078950173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7801/60000  13% ETA:  9:35:05 |\\\\\\\\                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 7800 is 0.000232249270988339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7901/60000  13% ETA:  9:33:40 |////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 7900 is 0.00017310354980624865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8001/60000  13% ETA:  9:32:38 |\\\\\\\\                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 8000 is 0.0002882362097003271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8101/60000  13% ETA:  9:31:20 |////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 8100 is 0.0003213024701718266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8201/60000  13% ETA:  9:30:05 |\\\\\\\\\\                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 8200 is 0.00032947856619632785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8301/60000  13% ETA:  9:29:01 |/////                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 8300 is 0.0003136533264054342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8401/60000  14% ETA:  9:27:53 |\\\\\\\\\\                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 8400 is 0.00029820040139611674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8501/60000  14% ETA:  9:26:45 |/////                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 8500 is 0.00034705285869089203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8601/60000  14% ETA:  9:26:20 |/////                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 8600 is 0.00040052886607026714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8701/60000  14% ETA:  9:25:19 ||||||                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 8700 is 0.0003959779209526015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8801/60000  14% ETA:  9:24:19 |-----                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 8800 is 0.00045332004371710324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 8901/60000  14% ETA:  9:23:13 ||||||                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 8900 is 0.0004371738391581237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9000/60000  15% ETA:  9:22:17 |-----                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 9000 is 0.0004433580646499109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9100/60000  15% ETA:  9:22:03 |/////                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 9100 is 0.000399956717737381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9200/60000  15% ETA:  9:20:52 |\\\\\\\\\\                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 9200 is 0.0003687777416194822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9300/60000  15% ETA:  9:19:41 |/////                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 9300 is 0.0003329434941983733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9400/60000  15% ETA:  9:18:29 |\\\\\\\\\\                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 9400 is 0.0003241765926576466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9500/60000  15% ETA:  9:17:35 |/////                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 9500 is 0.0003051767340101633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9600/60000  16% ETA:  9:16:23 |\\\\\\\\\\                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 9600 is 0.0002968768004445904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9700/60000  16% ETA:  9:15:13 |/////                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 9700 is 0.00029843912463995917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9801/60000  16% ETA:  9:14:20 |||||||                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 9800 is 0.00024492494412403903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9900/60000  16% ETA:  9:13:20 |------                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 100 avg reward for episode ending 9900 is 0.00020247994044976476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 9906/60000  16% ETA:  9:13:15 |//////                               | \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-cbdb97511194>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# explore = only explore for a certain number of episodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# action input needs to be transposed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mnoise_reduction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python Projects\\ReinforcementLearning\\Udacity_DRLN\\p3_collab-compet\\maddpg.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, obs_all_agents, noise, rand, eval_only)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_all_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;34m\"\"\"get actions from all agents in the MADDPG object\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_only\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaddpg_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_all_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python Projects\\ReinforcementLearning\\Udacity_DRLN\\p3_collab-compet\\maddpg.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_all_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;34m\"\"\"get actions from all agents in the MADDPG object\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_only\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaddpg_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_all_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python Projects\\ReinforcementLearning\\Udacity_DRLN\\p3_collab-compet\\ddpg.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, obs, noise, rand, eval_only)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0meval_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python Projects\\ReinforcementLearning\\Udacity_DRLN\\p3_collab-compet\\networkforall.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "# show progressbar\n",
    "import progressbar as pb\n",
    "widget = ['episode: ', pb.Counter(),'/',str(number_of_episodes),' ', \n",
    "          pb.Percentage(), ' ', pb.ETA(), ' ', pb.Bar(marker=pb.RotatingMarker()), ' ' ]\n",
    "\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=number_of_episodes).start()\n",
    "\n",
    "scores_deque = deque(np.zeros(100))\n",
    "rand = 1.0\n",
    "t = 0\n",
    "\n",
    "for episode in range(0, number_of_episodes):\n",
    "\n",
    "    timer.update(episode)\n",
    "\n",
    "    reward_this_episode = np.zeros(num_agents)\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    obs = env_info.vector_observations\n",
    "\n",
    "    #for calculating rewards for this particular episode - addition of all time steps\n",
    "\n",
    "    # save info or not\n",
    "    save_info = ((episode) % save_interval == 0 or episode==number_of_episodes-1)\n",
    "    #frames = []\n",
    "\n",
    "    #for episode_t in range(episode_length):\n",
    "    while True:\n",
    "        if t > 2000:\n",
    "            rand *= .9995\n",
    "        t += 1\n",
    "\n",
    "        # explore = only explore for a certain number of episodes\n",
    "        # action input needs to be transposed\n",
    "        actions = maddpg.act(torch.tensor(obs, dtype=torch.float), noise=noise, rand=rand, eval_only=True)\n",
    "        noise *= noise_reduction\n",
    "\n",
    "        actions = torch.stack(actions).detach().numpy()\n",
    "\n",
    "        # step forward one frame\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_obs = env_info.vector_observations            # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    " \n",
    "        # add data to buffer\n",
    "        transition = (obs, actions, rewards, next_obs, dones)\n",
    "        buffer.push(*transition)\n",
    "\n",
    "        reward_this_episode += rewards\n",
    "\n",
    "        obs = next_obs\n",
    "        \n",
    "        if np.any(dones):\n",
    "            break\n",
    "        \n",
    "    scores_deque.append(np.mean(reward_this_episode))\n",
    "    \n",
    "    # update once after every episode_per_update\n",
    "    if len(buffer) > batchsize and episode % episode_per_update == 0:\n",
    "        for _ in range(5):  # train for 5 times\n",
    "            samples = buffer.sample(batchsize)\n",
    "            for a_i in range(num_agents):\n",
    "                #samples = buffer.sample(batchsize)\n",
    "                maddpg.update(samples, a_i, logger)\n",
    "            maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "\n",
    "    agent0_reward.append(reward_this_episode[0])\n",
    "    agent1_reward.append(reward_this_episode[1])\n",
    "\n",
    "    if episode % 100 == 0 or episode == number_of_episodes-1:\n",
    "        avg_rewards = [np.mean(agent0_reward), np.mean(agent1_reward)]\n",
    "        agent0_reward = []\n",
    "        agent1_reward = []\n",
    "        for a_i, avg_rew in enumerate(avg_rewards):\n",
    "            logger.add_scalar('agent%i/mean_episode_rewards' % a_i, avg_rew, episode)\n",
    "    if episode %100 == 0:\n",
    "        print('last 100 avg reward for episode ending {} is {}'.format(episode, np.mean(scores_deque)))\n",
    "\n",
    "    #saving model\n",
    "    save_dict_list =[]\n",
    "    if save_info:\n",
    "        for i in range(2):\n",
    "\n",
    "            save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                         'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                         'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                         'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "            save_dict_list.append(save_dict)\n",
    "\n",
    "            torch.save(save_dict_list, \n",
    "                       os.path.join(model_dir, 'episode-{}-2.pt'.format(episode)))\n",
    "\n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\n",
    "            \"observation\", \"action\", \"reward\", \"next_observation\", \"done\"])\n",
    "    \n",
    "    def add(self, observation, action, reward, next_observation, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        \n",
    "        # Join a sequence of agents's states, next states and actions along columns\n",
    "        e = self.experience(observation, action, reward, next_observation, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        observations = torch.from_numpy(np.vstack([e.observation for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_observations = torch.from_numpy(np.vstack([e.next_observation for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        return (observations, actions, rewards, next_observations, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNoise:\n",
    "    \"\"\"uniformly distributed random noise process\"\"\"\n",
    "\n",
    "    def __init__(self, shape, amplitude):\n",
    "        \"\"\"Initialize parameters and noise process\n",
    "        Params\n",
    "        ======\n",
    "            shape (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            amplitude (int): size of each training batch\n",
    "        \"\"\"\n",
    "        \n",
    "        self.amplitude = amplitude\n",
    "        self.shape = shape\n",
    "        self.state = np.zeros(self.shape)\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to zero.\"\"\"\n",
    "        self.state = np.zeros(self.shape)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Return a noise sample.\"\"\"\n",
    "        self.state = self.amplitude*(2.*np.random.rand(self.shape) - 1.)\n",
    "        return self.state\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        np.random.seed(seed)\n",
    "        self.seed = np.random.randint(0,100)\n",
    "        self.size = size\n",
    "        self.reset()  \n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128, percent_dropout = 0.1):    \n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "            percent_dropout (float): percentage of nodes being dropped out.\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.layer_1 = nn.Sequential(nn.Linear(state_size, fc1_units),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(percent_dropout)) \n",
    "        \n",
    "        self.layer_2 = nn.Sequential(nn.Linear(fc1_units, fc2_units), \n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(percent_dropout))\n",
    "        \n",
    "        self.layer_3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # Apply to layers the specified weight initialization\n",
    "        self.layer_1[0].weight.data.uniform_(*hidden_init(self.layer_1[0]))\n",
    "        self.layer_2[0].weight.data.uniform_(*hidden_init(self.layer_2[0]))\n",
    "        self.layer_3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"           \n",
    "        x = self.layer_1(state)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        return torch.tanh(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128, percent_dropout = 0.1): \n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): seed\n",
    "            num_agents (int): Total number of agents\n",
    "            fc1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.layer_1 = nn.Sequential(nn.Linear(state_size * NUM_AGENTS + action_size * NUM_AGENTS, fc1_units),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(percent_dropout))\n",
    "        \n",
    "        self.layer_2 = nn.Sequential(nn.Linear(fc1_units, fc2_units),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(percent_dropout))\n",
    "               \n",
    "        self.layer_3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()       \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # Apply to layers the specified weight initialization\n",
    "        self.layer_1[0].weight.data.uniform_(*hidden_init(self.layer_1[0]))\n",
    "        self.layer_2[0].weight.data.uniform_(*hidden_init(self.layer_2[0]))\n",
    "        self.layer_3.weight.data.uniform_(-3e-3, 3e-3)  \n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-value.\"\"\"\n",
    "        xs = torch.cat((state, action), dim = 1)\n",
    "        x = self.layer_1(xs)\n",
    "        x = self.layer_2(x)\n",
    "        return self.layer_3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_name, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            agent_name (str): name of the agent\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random_seed\n",
    "        self.agent_name = agent_name\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, self.seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, self.seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, self.seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, self.seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_decay_rate = 0.999\n",
    "        self.epsilon_min = 0.2\n",
    "\n",
    "        # Noise process\n",
    "#         self.noise = OUNoise(action_size, random_seed)\n",
    "        self.noise = RNoise(action_size, 0.5)\n",
    "    def epsilon_decay(self):\n",
    "        self.epsilon = max(self.epsilon_decay_rate*self.epsilon, self.epsilon_min)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        if (type(state) != torch.Tensor):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.epsilon * self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "#     def learn(self, agent_name, experiences, gamma):\n",
    "    def learn(self, agent_name, my_next_observation, other_next_action, next_observations,\n",
    "              actions, observations, self_observation, other_pred_action, my_reward, my_done, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r +  * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            agent_name (str): name of the agent\n",
    "            my_next_observation (torch.Tensor): current agent's own next observation\n",
    "            other_next_action (torch.Tensor): other agents' actions **\n",
    "            next_observations (torch.Tensor): god-view next observation\n",
    "            actions (torch.Tensor): god-view observations\n",
    "            observations (torch.Tensor): god-view observations\n",
    "            self_observation (torch.Tensor): current agent's own observation\n",
    "            other_pred_action (torch.Tensor): other agents' predicted actions\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        \n",
    "        next_action = self.actor_target(my_next_observation)\n",
    "        if agent_name == 'agent_0':\n",
    "            next_actions = torch.cat(\n",
    "                           [next_action, other_next_action],\n",
    "                           1).to(device)\n",
    "        else:\n",
    "            next_actions = torch.cat(\n",
    "                           [other_next_action, next_action],\n",
    "                           1).to(device)\n",
    "        \n",
    "        Q_targets_next = self.critic_target(next_observations, next_actions)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = my_reward + (gamma * Q_targets_next * (1 - my_done))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(observations, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "#         Compute actor loss\n",
    "        pred_action = self.actor_local(self_observation)\n",
    "        \n",
    "        if agent_name == 'agent_0':\n",
    "            pred_actions = torch.cat(\n",
    "                           [pred_action, other_pred_action],\n",
    "                           1).to(device)\n",
    "        else:\n",
    "            pred_actions = torch.cat(\n",
    "                           [other_pred_action, pred_action],\n",
    "                           1).to(device)\n",
    "        \n",
    "        actor_loss = -self.critic_local(observations, pred_actions).mean()\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        _target = *_local + (1 - )*_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG_Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed = 0):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            num_agents (int): how many agents to be trained\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = random_seed\n",
    "\n",
    "        self.memory = Buffer(BUFFER_SIZE, BATCH_SIZE, self.seed)\n",
    "\n",
    "        self.agent_names = []\n",
    "        for i in range(num_agents):\n",
    "            self.agent_names.append( 'agent_' + str(i) )\n",
    "\n",
    "        self.agents = dict()\n",
    "        for agent_name in self.agent_names:\n",
    "            self.agents[agent_name] = DDPG_Agent(agent_name, state_size, action_size, self.seed)\n",
    "\n",
    "    def act(self, observations, add_noise = True):\n",
    "        '''get actions for both agents\n",
    "        Params\n",
    "        ======\n",
    "            observations (np.array): current observation\n",
    "            add_noise (bool): add noise or not\n",
    "        '''\n",
    "        actions = []\n",
    "        for i, agent_name in enumerate(self.agent_names):\n",
    "            actions.append(self.agents[agent_name].act(observations[i], add_noise))\n",
    "        return np.array(actions)\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        '''reset the noise class'''\n",
    "        for agent_name in self.agent_names:\n",
    "            self.agents[agent_name].reset()\n",
    "    \n",
    "    def epsilon_decay(self):\n",
    "        '''Decay the noise amplitude if required'''\n",
    "        for agent_name in self.agent_names:\n",
    "            self.agents[agent_name].epsilon_decay()\n",
    "            \n",
    "    def step(self, observation, action, reward, next_observation, done, step):\n",
    "        \"\"\"Learning process, get past experience tuple in the replay buffer,\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            observation (torch.Tensor): all agents' observations\n",
    "            action (torch.Tensor): all agents' observations\n",
    "            next_observations (torch.Tensor): all agents' next observation\n",
    "            observations (torch.Tensor): all agents' observations\n",
    "            done (torch.Tensor): all agents' dones\n",
    "            step (int): current training step, use it for noise decay\n",
    "        \"\"\"\n",
    "        self.memory.add(observation, action, reward, next_observation, done)\n",
    "        if (len(self.memory) > BATCH_SIZE) and (step % TRAIN_EVERY) == 0:\n",
    "            for _ in range(NUM_TRAINS) :\n",
    "                experiences = self.memory.sample()\n",
    "                observations, actions, rewards, next_observations, dones = experiences\n",
    "\n",
    "                my_observation = torch.chunk(observations, NUM_AGENTS, dim = 1)\n",
    "                my_next_observation = torch.chunk(next_observations, NUM_AGENTS, dim = 1)\n",
    "                my_reward = torch.chunk(rewards, NUM_AGENTS, dim = 1)\n",
    "                my_done = torch.chunk(dones, NUM_AGENTS, dim = 1)\n",
    "\n",
    "                other_next_actions = []\n",
    "                other_pred_actions = []\n",
    "                # prepare next step actions for actor learning process. The date will be fed in critic_local\n",
    "                for num_agent, agent_name in enumerate(self.agent_names):\n",
    "                    other_next_actions.append( torch.Tensor(self.agents[agent_name].act(my_next_observation[num_agent])) )\n",
    "                    other_pred_actions.append( torch.Tensor(self.agents[agent_name].act(my_observation[num_agent])) )\n",
    "\n",
    "                self.agents['agent_0'].learn('agent_0', my_next_observation[0], other_next_actions[1], next_observations,\n",
    "                  actions, observations, my_observation[0], other_pred_actions[1], my_reward[0], my_done[0], gamma = GAMMA)\n",
    "                self.agents['agent_1'].learn('agent_1', my_next_observation[1], other_next_actions[0], next_observations,\n",
    "                  actions, observations, my_observation[1], other_pred_actions[0], my_reward[1], my_done[1], gamma = GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 256        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "NUM_AGENTS = 2          # number of agents\n",
    "WEIGHT_DECAY = 0.       # L2 weight decay\n",
    "TRAIN_EVERY = 1         # how often to train the network\n",
    "NUM_TRAINS = 5          # number of trains per each train step\n",
    "\n",
    "agent = MADDPG_Agent(state_size, action_size, num_agents, random_seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage score: 0.0010\tScore: 0.0000\n",
      "Episode 200\tAverage score: 0.0114\tScore: 0.0000\n",
      "Episode 300\tAverage score: 0.0279\tScore: 0.1000\n",
      "Episode 316\tAverage Score: 0.0328\tScore: 0.0900"
     ]
    }
   ],
   "source": [
    "def maddpg(n_episodes=5000, score_lenth = 100 ):\n",
    "    \"\"\"Multi-Agent Deep Deterministic Policy Gradient for N agents\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "    \"\"\"\n",
    "    scores_deque = deque(maxlen=score_lenth)                              \n",
    "    scores = []                                                         \n",
    "    average_scores = []\n",
    "    for i_episode in range(1, n_episodes+1):                                    \n",
    "        env_info = env.reset(train_mode=True)[brain_name]                   \n",
    "        states = env_info.vector_observations                                          \n",
    "        observation = states.reshape(1,NUM_AGENTS*state_size).squeeze(0)  # merge both agents' states as an observation\n",
    "        score = np.zeros(num_agents)                                    \n",
    "        agent.reset()\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            actions = agent.act(states)\n",
    "            action = actions.reshape(1,NUM_AGENTS*action_size).squeeze(0) # merge both agents' actions as an action\n",
    "            env_info = env.step(actions)[brain_name]                                  \n",
    "            next_states = env_info.vector_observations                   \n",
    "            next_observation = next_states.reshape(1,NUM_AGENTS*state_size).squeeze(0) # merge both agents' next states as the next observation\n",
    "            rewards = env_info.rewards                                        \n",
    "            dones = env_info.local_done                                 \n",
    "            agent.step(observation, action, rewards, next_observation, dones, step)   \n",
    "            states = next_states                                        \n",
    "            observation = next_observation\n",
    "            score += rewards                                             \n",
    "            if any(dones):                                 \n",
    "                break                                                   \n",
    "#         agent.epsilon_decay()\n",
    "        score = np.max(score)\n",
    "        scores.append(score)\n",
    "        scores_deque.append(score)\n",
    "        average_score = np.mean(scores_deque)\n",
    "        average_scores.append(average_score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.4f}\\tScore: {:.4f}'.format(i_episode, average_score, score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage score: {:.4f}'.format(i_episode , average_score))\n",
    "        if average_score >= 0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.4f}'.format(i_episode, average_score))\n",
    "            break\n",
    "    \n",
    "    torch.save(agent.agents['agent_0'].actor_local.state_dict(), 'agent_one_checkpoint_actor.pth')\n",
    "    torch.save(agent.agents['agent_0'].critic_local.state_dict(), 'agent_one_checkpoint_critic.pth')\n",
    "\n",
    "    torch.save(agent.agents['agent_1'].actor_local.state_dict(), 'agent_two_checkpoint_actor.pth')\n",
    "    torch.save(agent.agents['agent_1'].critic_local.state_dict(), 'agent_two_checkpoint_critic.pth')\n",
    "    \n",
    "    return scores, average_scores            \n",
    "\n",
    "scores, average_scores = maddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
