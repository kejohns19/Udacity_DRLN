{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\n",
    "    '<style>'\n",
    "        '#notebook { padding-top:0px !important; } ' \n",
    "        '.container { width:95% !important; } '\n",
    "        '.end_space { min-height:0px !important; } '\n",
    "    '</style>'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "#env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\")\n",
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from buffer import ReplayBuffer\n",
    "from common.Memory import ReplayMemory\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor\n",
    "from collections import deque\n",
    "\n",
    "# keep training awake\n",
    "#from workspace_utils import keep_awake\n",
    "\n",
    "# for saving gif\n",
    "#import imageio\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# number of training episodes.\n",
    "# change this to higher number to experiment. say 30000.\n",
    "number_of_episodes = 60000\n",
    "episode_length = 200\n",
    "batchsize = 256\n",
    "# how many episodes to save policy and gif\n",
    "save_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "#noise = 2\n",
    "#noise_reduction = 0.999\n",
    "\n",
    "log_path = os.getcwd()+\"/log\"\n",
    "model_dir= os.getcwd()+\"/model_dir\"\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# keep 5000 episodes worth of replay\n",
    "buffer = ReplayMemory(int(1e5))\n",
    "\n",
    "# initialize policy and critic\n",
    "\n",
    "in_actor = state_size\n",
    "hidden_in_actor = 256\n",
    "hidden_out_actor = 128\n",
    "out_actor = 2\n",
    "# critic input = obs from both agents + actions from both agents\n",
    "in_critic = 2*state_size + 2*action_size\n",
    "hidden_in_critic = 256\n",
    "hidden_out_critic = 128\n",
    "\n",
    "maddpg = MADDPG(in_actor, hidden_in_actor, hidden_out_actor, \n",
    "                out_actor, in_critic, hidden_in_critic, hidden_out_critic,\n",
    "                lr_actor=1.0e-4, lr_critic=1.0e-3, discount_factor=0.99, tau=1.0e-3)\n",
    "\n",
    "logger = SummaryWriter(log_dir=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many episodes before update\n",
    "steps_per_update = 1\n",
    "num_updates = 4\n",
    "random_actions = 8000\n",
    "update_start = 8000\n",
    "\n",
    "agent0_reward = []\n",
    "agent1_reward = []\n",
    "average_score_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.0190\tScore: 0.0000\tAvg Ep Len: 10.60\n",
      "Episode 200\tAverage Score: 0.0274\tScore: 0.0000\tAvg Ep Len: 14.56\n",
      "Episode 300\tAverage Score: 0.0325\tScore: 0.0000\tAvg Ep Len: 16.72\n",
      "Episode 400\tAverage Score: 0.0332\tScore: 0.2000\tAvg Ep Len: 17.59\n",
      "Episode 500\tAverage Score: 0.0317\tScore: 0.0000\tAvg Ep Len: 17.84\n",
      "Episode 600\tAverage Score: 0.0300\tScore: 0.0000\tAvg Ep Len: 17.91\n",
      "Episode 700\tAverage Score: 0.0289\tScore: 0.0000\tAvg Ep Len: 18.04\n",
      "Episode 800\tAverage Score: 0.0266\tScore: 0.0000\tAvg Ep Len: 17.87\n",
      "Episode 900\tAverage Score: 0.0259\tScore: 0.0000\tAvg Ep Len: 18.01\n",
      "Episode 1000\tAverage Score: 0.0252\tScore: 0.0000\tAvg Ep Len: 17.98\n",
      "Episode 1100\tAverage Score: 0.0250\tScore: 0.0000\tAvg Ep Len: 18.09\n",
      "Episode 1200\tAverage Score: 0.0246\tScore: 0.0000\tAvg Ep Len: 18.12\n",
      "Episode 1300\tAverage Score: 0.0240\tScore: 0.0000\tAvg Ep Len: 18.09\n",
      "Episode 1400\tAverage Score: 0.0232\tScore: 0.0000\tAvg Ep Len: 17.96\n",
      "Episode 1456\tAverage Score: 0.0230\tScore: 0.0000\tAvg Ep Len: 17.99"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7736f3bde60c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ma_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[1;31m#samples = buffer.sample(batchsize)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m                     \u001b[0mmaddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m                 \u001b[0mmaddpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#soft update the target network towards the actual networks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Python Projects\\ReinforcementLearning\\Udacity_DRLN\\p3_collab-compet\\maddpg.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, samples, agent_number, logger)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# get the policy gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_input2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mactor_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;31m#torch.nn.utils.clip_grad_norm_(agent.actor.parameters(),0.5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "# show progressbar\n",
    "#import progressbar as pb\n",
    "#widget = ['episode: ', pb.Counter(),'/',str(number_of_episodes),' ', \n",
    "#          pb.Percentage(), ' ', pb.ETA(), ' ', pb.Bar(marker=pb.RotatingMarker()), ' ' ]\n",
    "\n",
    "#timer = pb.ProgressBar(widgets=widget, maxval=number_of_episodes).start()\n",
    "\n",
    "scores_deque = deque(np.zeros(100))\n",
    "ep_len_deque = deque(np.zeros(100))\n",
    "#rand = 1.0\n",
    "t = 0\n",
    "\n",
    "for episode in range(1, number_of_episodes+1):\n",
    "\n",
    "    #timer.update(episode)\n",
    "\n",
    "    reward_this_episode = np.zeros(num_agents)\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    obs = env_info.vector_observations\n",
    "    ep_len = 0\n",
    "\n",
    "    #for calculating rewards for this particular episode - addition of all time steps\n",
    "\n",
    "    # save info or not\n",
    "    save_info = ((episode) % save_interval == 0 or episode==number_of_episodes)\n",
    "    #frames = []\n",
    "    maddpg.reset()\n",
    "\n",
    "    #for episode_t in range(episode_length):\n",
    "    while True:\n",
    "        if t > random_actions:\n",
    "            rand = 1.0\n",
    "        else:\n",
    "            rand = 0.0\n",
    "        t += 1\n",
    "        ep_len += 1\n",
    "\n",
    "        # explore = only explore for a certain number of episodes\n",
    "        # action input needs to be transposed\n",
    "        actions = maddpg.act(torch.tensor(obs, dtype=torch.float), rand=rand, add_noise=True)\n",
    "        #noise *= noise_reduction\n",
    "\n",
    "        actions = torch.stack(actions).detach().numpy()\n",
    "\n",
    "        # step forward one frame\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_obs = env_info.vector_observations            # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    " \n",
    "        # add data to buffer\n",
    "        transition = (obs, actions, rewards, next_obs, dones)\n",
    "        buffer.push(*transition)\n",
    "        \n",
    "        # update once after every episode_per_update\n",
    "        if t > update_start and t % steps_per_update == 0:\n",
    "            for _ in range(steps_per_update * num_updates):  # train for x times per update\n",
    "                samples = buffer.sample(batchsize)\n",
    "                for a_i in range(num_agents):\n",
    "                    #samples = buffer.sample(batchsize)\n",
    "                    maddpg.update(samples, a_i, logger)\n",
    "                maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "\n",
    "        reward_this_episode += rewards\n",
    "\n",
    "        obs = next_obs\n",
    "        \n",
    "        if np.any(dones):\n",
    "            break\n",
    "    ep_len_deque.append(ep_len)\n",
    "    avg_ep_len = np.mean(ep_len_deque)\n",
    "    score = np.max(reward_this_episode)\n",
    "    scores_deque.append(score)\n",
    "    average_score = np.mean(scores_deque)\n",
    "    average_score_log.append(average_score)\n",
    "    '''\n",
    "    # update once after every episode_per_update\n",
    "    if len(buffer) > batchsize and episode % episode_per_update == 0:\n",
    "        for _ in range(5):  # train for 5 times\n",
    "            samples = buffer.sample(batchsize)\n",
    "            for a_i in range(num_agents):\n",
    "                #samples = buffer.sample(batchsize)\n",
    "                maddpg.update(samples, a_i, logger)\n",
    "            maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "    '''\n",
    "    agent0_reward.append(reward_this_episode[0])\n",
    "    agent1_reward.append(reward_this_episode[1])\n",
    "\n",
    "    if episode % 100 == 0 or episode == number_of_episodes-1:\n",
    "        avg_rewards = [np.mean(agent0_reward), np.mean(agent1_reward)]\n",
    "        agent0_reward = []\n",
    "        agent1_reward = []\n",
    "        for a_i, avg_rew in enumerate(avg_rewards):\n",
    "            logger.add_scalar('agent%i/mean_episode_rewards' % a_i, avg_rew, episode)\n",
    "            \n",
    "    print('\\rEpisode {}\\tAverage Score: {:.4f}\\tScore: {:.4f}\\tAvg Ep Len: {:.2f}'.format(episode, average_score, score, avg_ep_len), end=\"\")\n",
    "    if episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.4f}\\tScore: {:.4f}\\tAvg Ep Len: {:.2f}'.format(episode, average_score, score, avg_ep_len))\n",
    "    if average_score >= 0.5:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.4f}'.format(episode, average_score))\n",
    "        break\n",
    "    '''\n",
    "    if episode %100 == 0:\n",
    "        print('last 100 avg reward for episode ending {} is {}'.format(episode, np.mean(scores_deque)))\n",
    "    '''\n",
    "    #saving model\n",
    "    \n",
    "    if save_info:\n",
    "        save_dict_list =[]\n",
    "        for i in range(2):\n",
    "            save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                         'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                         'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                         'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "            save_dict_list.append(save_dict)\n",
    "\n",
    "        torch.save(save_dict_list, os.path.join(model_dir, 'episode-{}.pt'.format(episode)))\n",
    "\n",
    "#timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\n",
    "            \"observation\", \"action\", \"reward\", \"next_observation\", \"done\"])\n",
    "    \n",
    "    def add(self, observation, action, reward, next_observation, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        \n",
    "        # Join a sequence of agents's states, next states and actions along columns\n",
    "        e = self.experience(observation, action, reward, next_observation, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        observations = torch.from_numpy(np.vstack([e.observation for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_observations = torch.from_numpy(np.vstack([e.next_observation for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        return (observations, actions, rewards, next_observations, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNoise:\n",
    "    \"\"\"uniformly distributed random noise process\"\"\"\n",
    "\n",
    "    def __init__(self, shape, amplitude):\n",
    "        \"\"\"Initialize parameters and noise process\n",
    "        Params\n",
    "        ======\n",
    "            shape (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            amplitude (int): size of each training batch\n",
    "        \"\"\"\n",
    "        \n",
    "        self.amplitude = amplitude\n",
    "        self.shape = shape\n",
    "        self.state = np.zeros(self.shape)\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to zero.\"\"\"\n",
    "        self.state = np.zeros(self.shape)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Return a noise sample.\"\"\"\n",
    "        self.state = self.amplitude*(2.*np.random.rand(self.shape) - 1.)\n",
    "        return self.state\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        np.random.seed(seed)\n",
    "        self.seed = np.random.randint(0,100)\n",
    "        self.size = size\n",
    "        self.reset()  \n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128, percent_dropout = 0.1):    \n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "            percent_dropout (float): percentage of nodes being dropped out.\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.layer_1 = nn.Sequential(nn.Linear(state_size, fc1_units),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(percent_dropout)) \n",
    "        \n",
    "        self.layer_2 = nn.Sequential(nn.Linear(fc1_units, fc2_units), \n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(percent_dropout))\n",
    "        \n",
    "        self.layer_3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # Apply to layers the specified weight initialization\n",
    "        self.layer_1[0].weight.data.uniform_(*hidden_init(self.layer_1[0]))\n",
    "        self.layer_2[0].weight.data.uniform_(*hidden_init(self.layer_2[0]))\n",
    "        self.layer_3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"           \n",
    "        x = self.layer_1(state)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        return torch.tanh(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128, percent_dropout = 0.1): \n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): seed\n",
    "            num_agents (int): Total number of agents\n",
    "            fc1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.layer_1 = nn.Sequential(nn.Linear(state_size * NUM_AGENTS + action_size * NUM_AGENTS, fc1_units),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(percent_dropout))\n",
    "        \n",
    "        self.layer_2 = nn.Sequential(nn.Linear(fc1_units, fc2_units),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(percent_dropout))\n",
    "               \n",
    "        self.layer_3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()       \n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # Apply to layers the specified weight initialization\n",
    "        self.layer_1[0].weight.data.uniform_(*hidden_init(self.layer_1[0]))\n",
    "        self.layer_2[0].weight.data.uniform_(*hidden_init(self.layer_2[0]))\n",
    "        self.layer_3.weight.data.uniform_(-3e-3, 3e-3)  \n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-value.\"\"\"\n",
    "        xs = torch.cat((state, action), dim = 1)\n",
    "        x = self.layer_1(xs)\n",
    "        x = self.layer_2(x)\n",
    "        return self.layer_3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_name, state_size, action_size, random_seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            agent_name (str): name of the agent\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random_seed\n",
    "        self.agent_name = agent_name\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, self.seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, self.seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, self.seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, self.seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_decay_rate = 0.999\n",
    "        self.epsilon_min = 0.2\n",
    "\n",
    "        # Noise process\n",
    "#         self.noise = OUNoise(action_size, random_seed)\n",
    "        self.noise = RNoise(action_size, 0.5)\n",
    "    def epsilon_decay(self):\n",
    "        self.epsilon = max(self.epsilon_decay_rate*self.epsilon, self.epsilon_min)\n",
    "\n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        if (type(state) != torch.Tensor):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.epsilon * self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "#     def learn(self, agent_name, experiences, gamma):\n",
    "    def learn(self, agent_name, my_next_observation, other_next_action, next_observations,\n",
    "              actions, observations, self_observation, other_pred_action, my_reward, my_done, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            agent_name (str): name of the agent\n",
    "            my_next_observation (torch.Tensor): current agent's own next observation\n",
    "            other_next_action (torch.Tensor): other agents' actions **\n",
    "            next_observations (torch.Tensor): god-view next observation\n",
    "            actions (torch.Tensor): god-view observations\n",
    "            observations (torch.Tensor): god-view observations\n",
    "            self_observation (torch.Tensor): current agent's own observation\n",
    "            other_pred_action (torch.Tensor): other agents' predicted actions\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        \n",
    "        next_action = self.actor_target(my_next_observation)\n",
    "        if agent_name == 'agent_0':\n",
    "            next_actions = torch.cat(\n",
    "                           [next_action, other_next_action],\n",
    "                           1).to(device)\n",
    "        else:\n",
    "            next_actions = torch.cat(\n",
    "                           [other_next_action, next_action],\n",
    "                           1).to(device)\n",
    "        \n",
    "        Q_targets_next = self.critic_target(next_observations, next_actions)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = my_reward + (gamma * Q_targets_next * (1 - my_done))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(observations, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "#         Compute actor loss\n",
    "        pred_action = self.actor_local(self_observation)\n",
    "        \n",
    "        if agent_name == 'agent_0':\n",
    "            pred_actions = torch.cat(\n",
    "                           [pred_action, other_pred_action],\n",
    "                           1).to(device)\n",
    "        else:\n",
    "            pred_actions = torch.cat(\n",
    "                           [other_pred_action, pred_action],\n",
    "                           1).to(device)\n",
    "        \n",
    "        actor_loss = -self.critic_local(observations, pred_actions).mean()\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG_Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed = 0):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            num_agents (int): how many agents to be trained\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.num_agents = num_agents\n",
    "        self.seed = random_seed\n",
    "\n",
    "        self.memory = Buffer(BUFFER_SIZE, BATCH_SIZE, self.seed)\n",
    "\n",
    "        self.agent_names = []\n",
    "        for i in range(num_agents):\n",
    "            self.agent_names.append( 'agent_' + str(i) )\n",
    "\n",
    "        self.agents = dict()\n",
    "        for agent_name in self.agent_names:\n",
    "            self.agents[agent_name] = DDPG_Agent(agent_name, state_size, action_size, self.seed)\n",
    "\n",
    "    def act(self, observations, add_noise = True):\n",
    "        '''get actions for both agents\n",
    "        Params\n",
    "        ======\n",
    "            observations (np.array): current observation\n",
    "            add_noise (bool): add noise or not\n",
    "        '''\n",
    "        actions = []\n",
    "        for i, agent_name in enumerate(self.agent_names):\n",
    "            actions.append(self.agents[agent_name].act(observations[i], add_noise))\n",
    "        return np.array(actions)\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        '''reset the noise class'''\n",
    "        for agent_name in self.agent_names:\n",
    "            self.agents[agent_name].reset()\n",
    "    \n",
    "    def epsilon_decay(self):\n",
    "        '''Decay the noise amplitude if required'''\n",
    "        for agent_name in self.agent_names:\n",
    "            self.agents[agent_name].epsilon_decay()\n",
    "            \n",
    "    def step(self, observation, action, reward, next_observation, done, step):\n",
    "        \"\"\"Learning process, get past experience tuple in the replay buffer,\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            observation (torch.Tensor): all agents' observations\n",
    "            action (torch.Tensor): all agents' observations\n",
    "            next_observations (torch.Tensor): all agents' next observation\n",
    "            observations (torch.Tensor): all agents' observations\n",
    "            done (torch.Tensor): all agents' dones\n",
    "            step (int): current training step, use it for noise decay\n",
    "        \"\"\"\n",
    "        self.memory.add(observation, action, reward, next_observation, done)\n",
    "        if (len(self.memory) > BATCH_SIZE) and (step % TRAIN_EVERY) == 0:\n",
    "            for _ in range(NUM_TRAINS) :\n",
    "                experiences = self.memory.sample()\n",
    "                observations, actions, rewards, next_observations, dones = experiences\n",
    "\n",
    "                my_observation = torch.chunk(observations, NUM_AGENTS, dim = 1)\n",
    "                my_next_observation = torch.chunk(next_observations, NUM_AGENTS, dim = 1)\n",
    "                my_reward = torch.chunk(rewards, NUM_AGENTS, dim = 1)\n",
    "                my_done = torch.chunk(dones, NUM_AGENTS, dim = 1)\n",
    "\n",
    "                other_next_actions = []\n",
    "                other_pred_actions = []\n",
    "                # prepare next step actions for actor learning process. The date will be fed in critic_local\n",
    "                for num_agent, agent_name in enumerate(self.agent_names):\n",
    "                    other_next_actions.append( torch.Tensor(self.agents[agent_name].act(my_next_observation[num_agent])) )\n",
    "                    other_pred_actions.append( torch.Tensor(self.agents[agent_name].act(my_observation[num_agent])) )\n",
    "\n",
    "                self.agents['agent_0'].learn('agent_0', my_next_observation[0], other_next_actions[1], next_observations,\n",
    "                  actions, observations, my_observation[0], other_pred_actions[1], my_reward[0], my_done[0], gamma = GAMMA)\n",
    "                self.agents['agent_1'].learn('agent_1', my_next_observation[1], other_next_actions[0], next_observations,\n",
    "                  actions, observations, my_observation[1], other_pred_actions[0], my_reward[1], my_done[1], gamma = GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 256        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "NUM_AGENTS = 2          # number of agents\n",
    "WEIGHT_DECAY = 0.       # L2 weight decay\n",
    "TRAIN_EVERY = 1         # how often to train the network\n",
    "NUM_TRAINS = 5          # number of trains per each train step\n",
    "\n",
    "agent = MADDPG_Agent(state_size, action_size, num_agents, random_seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def maddpg(n_episodes=5000, score_lenth = 100 ):\n",
    "    \"\"\"Multi-Agent Deep Deterministic Policy Gradient for N agents\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "    \"\"\"\n",
    "    scores_deque = deque(maxlen=score_lenth)                              \n",
    "    scores = []                                                         \n",
    "    average_scores = []\n",
    "    for i_episode in range(1, n_episodes+1):                                    \n",
    "        env_info = env.reset(train_mode=True)[brain_name]                   \n",
    "        states = env_info.vector_observations                                          \n",
    "        observation = states.reshape(1,NUM_AGENTS*state_size).squeeze(0)  # merge both agents' states as an observation\n",
    "        score = np.zeros(num_agents)                                    \n",
    "        agent.reset()\n",
    "        step = 0\n",
    "        while True:\n",
    "            step += 1\n",
    "            actions = agent.act(states)\n",
    "            action = actions.reshape(1,NUM_AGENTS*action_size).squeeze(0) # merge both agents' actions as an action\n",
    "            env_info = env.step(actions)[brain_name]                                  \n",
    "            next_states = env_info.vector_observations                   \n",
    "            next_observation = next_states.reshape(1,NUM_AGENTS*state_size).squeeze(0) # merge both agents' next states as the next observation\n",
    "            rewards = env_info.rewards                                        \n",
    "            dones = env_info.local_done                                 \n",
    "            agent.step(observation, action, rewards, next_observation, dones, step)   \n",
    "            states = next_states                                        \n",
    "            observation = next_observation\n",
    "            score += rewards                                             \n",
    "            if any(dones):                                 \n",
    "                break                                                   \n",
    "#         agent.epsilon_decay()\n",
    "        score = np.max(score)\n",
    "        scores.append(score)\n",
    "        scores_deque.append(score)\n",
    "        average_score = np.mean(scores_deque)\n",
    "        average_scores.append(average_score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.4f}\\tScore: {:.4f}'.format(i_episode, average_score, score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage score: {:.4f}'.format(i_episode , average_score))\n",
    "        if average_score >= 0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.4f}'.format(i_episode, average_score))\n",
    "            break\n",
    "    \n",
    "    torch.save(agent.agents['agent_0'].actor_local.state_dict(), 'agent_one_checkpoint_actor.pth')\n",
    "    torch.save(agent.agents['agent_0'].critic_local.state_dict(), 'agent_one_checkpoint_critic.pth')\n",
    "\n",
    "    torch.save(agent.agents['agent_1'].actor_local.state_dict(), 'agent_two_checkpoint_actor.pth')\n",
    "    torch.save(agent.agents['agent_1'].critic_local.state_dict(), 'agent_two_checkpoint_critic.pth')\n",
    "    \n",
    "    return scores, average_scores            \n",
    "\n",
    "scores, average_scores = maddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent0_rewards = np.random.randn(300)\n",
    "agent1_rewards = np.random.randn(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.max(np.stack((agent0_rewards, agent1_rewards)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[299]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(average_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
